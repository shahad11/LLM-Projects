{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## What are Prompt Templates?\n",
        "\n",
        "Prompt Templates in LangChain are **reusable frameworks for constructing prompts**. Think of them as blueprints where dynamic content (variables) can be inserted based on specific inputs. This allows you to generate consistent, well-structured prompts for different tasks without hardcoding every prompt.\n",
        "\n",
        "A Prompt Template is essentially a **string with placeholders** for variables that can be filled at runtime, creating a custom prompt based on the context or input provided.\n",
        "\n",
        "**Key Features:**\n",
        "\n",
        "**Reusability:** You can define a template once and reuse it across different tasks, keeping your codebase clean.\n",
        "\n",
        "**Dynamic Variables:** Variables can be inserted into the template, making it flexible for different inputs or conditions.\n",
        "\n",
        "**Consistency:** Ensures that prompts are structured uniformly, which is crucial when working with LLMs for consistent outputs.\n",
        "\n",
        "## Why Prompt Templates?\n",
        "In **natural language processing** (NLP) and working with **large language models** (LLMs), generating meaningful responses often relies on how well you design the input (prompt) given to the model. A slight change in the phrasing of the prompt can lead to vastly different outputs.\n",
        "\n",
        "### Challenges without Prompt Templates:\n",
        "\n",
        "Writing custom prompts for every task can be repetitive and error-prone.\n",
        "Manually constructing prompts for various inputs increases the complexity of your code.\n",
        "\n",
        "It's harder to maintain **consistency** in your application's logic when managing many different inputs.\n",
        "\n",
        "**Solution:** Prompt Templates LangChain introduces Prompt Templates to make prompting easier and more modular. It allows you to define the structure of your prompts, enabling you to **reuse** them across tasks while **dynamically inserting** the necessary variables.\n",
        "\n"
      ],
      "metadata": {
        "id": "n3soN-NHdbU8"
      },
      "id": "n3soN-NHdbU8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **How to Use Prompt Templates in LangChain**"
      ],
      "metadata": {
        "id": "SUKUXK9h8XgL"
      },
      "id": "SUKUXK9h8XgL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**01: Installation**"
      ],
      "metadata": {
        "id": "09CgA1RZkiC4"
      },
      "id": "09CgA1RZkiC4"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community\n",
        "from langchain import HuggingFaceHub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4tDdLTjkkk_",
        "outputId": "958cbf56-ef7f-43f9-ea27-95e9ba4226fd"
      },
      "id": "X4tDdLTjkkk_",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.6 (from langchain)\n",
            "  Downloading langchain_core-0.3.7-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.129-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.6->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.6->langchain) (4.12.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.6->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.3.1-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.3.7-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.129-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.2/292.2 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, python-dotenv, orjson, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, jsonpatch, httpcore, pydantic-settings, httpx, dataclasses-json, langsmith, langchain-core, langchain-text-splitters, langchain, langchain_community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.1 langchain-core-0.3.7 langchain-text-splitters-0.3.0 langchain_community-0.3.1 langsmith-0.1.129 marshmallow-3.22.0 mypy-extensions-1.0.0 orjson-3.10.7 pydantic-settings-2.5.2 python-dotenv-1.0.1 tenacity-8.5.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**02: Setup the Environment**"
      ],
      "metadata": {
        "id": "sQHZiF38-Cps"
      },
      "id": "sQHZiF38-Cps"
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "9-mFf0Ql-KX2"
      },
      "id": "9-mFf0Ql-KX2",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "f31c4cc6",
      "metadata": {
        "id": "f31c4cc6"
      },
      "outputs": [],
      "source": [
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"Your Hugging face API key\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hugging Face**"
      ],
      "metadata": {
        "id": "bj6wjnKZ-bgU"
      },
      "id": "bj6wjnKZ-bgU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Example 1**"
      ],
      "metadata": {
        "id": "iTsUW116-th1"
      },
      "id": "iTsUW116-th1"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDMLw7Yr-nQK",
        "outputId": "7ba0cb06-439e-417b-d21c-e576b01c709b"
      },
      "id": "hDMLw7Yr-nQK",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub"
      ],
      "metadata": {
        "id": "B4w0ultA-icd"
      },
      "id": "B4w0ultA-icd",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "0782a2dd",
      "metadata": {
        "id": "0782a2dd"
      },
      "source": [
        "## **Prompt Templates**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain faciliates prompt management and optimization.\n",
        "\n",
        "Normally when you use an LLM in an application, you are not sending user input directly to the LLM. Instead, you need to take the user input and construct a prompt, and only then send that to the LLM."
      ],
      "metadata": {
        "id": "jszTHb6J_dNV"
      },
      "id": "jszTHb6J_dNV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "In many Large Language Model applications we donot pass the user input directly to the Large Language Model, we add the user input to a large piece of text called prompt template"
      ],
      "metadata": {
        "id": "unU1DcEv7TWh"
      },
      "id": "unU1DcEv7TWh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Example 1**"
      ],
      "metadata": {
        "id": "IWqka6F_93QB"
      },
      "id": "IWqka6F_93QB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- A prompt template consists of a general structure where placeholders for variables are defined. These placeholders will later be filled in with actual data when generating the prompt."
      ],
      "metadata": {
        "id": "czgyAm6ZllB_"
      },
      "id": "czgyAm6ZllB_"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "7a306b9d",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7a306b9d",
        "outputId": "2b60c066-aeda-4edb-b694-606c7516715b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I want to open a restaurant for indian food. Suggest a fency name for this.\n"
          ]
        }
      ],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
        ")\n",
        "p = prompt_template_name.format(cuisine=\"indian\")\n",
        "print(p)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Example 2**"
      ],
      "metadata": {
        "id": "qlKeWd7B95-R"
      },
      "id": "qlKeWd7B95-R"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "prompt = PromptTemplate.from_template(\"What is a good name for a company that makes {product}\")\n",
        "#Once the prompt template is defined, you can fill it with actual values when you're ready to generate the prompt.\n",
        "prompt.format(product=\"colorful socks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qqJZBS9u8534",
        "outputId": "3c7617b6-fc21-43ab-b7ec-4641f013b875"
      },
      "id": "qqJZBS9u8534",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'What is a good name for a company that makes colorful socks'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O3YEt1YZKJHr"
      },
      "id": "O3YEt1YZKJHr",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using with LLMs"
      ],
      "metadata": {
        "id": "TgfcUbLil6x_"
      },
      "id": "TgfcUbLil6x_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Example 1**"
      ],
      "metadata": {
        "id": "o3sX4p-xneLZ"
      },
      "id": "o3sX4p-xneLZ"
    },
    {
      "cell_type": "code",
      "source": [
        "llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":0, \"max_length\":64})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFjkhSBfbw4Q",
        "outputId": "da770165-c91d-4f1b-be6b-1decb24eae50"
      },
      "id": "YFjkhSBfbw4Q",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-faa71cef28ea>:1: LangChainDeprecationWarning: The class `HuggingFaceHub` was deprecated in LangChain 0.0.21 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEndpoint``.\n",
            "  llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":0, \"max_length\":64})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using Prompts in a chain"
      ],
      "metadata": {
        "id": "O7EeYTUVcShS"
      },
      "id": "O7EeYTUVcShS"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- When this template is used, it will dynamically insert the value of cuisine into the {cuisine} placeholder. For example, if you input \"Italian\" as the cuisine, the prompt will become:"
      ],
      "metadata": {
        "id": "lkRkIw6LmyKt"
      },
      "id": "lkRkIw6LmyKt"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
        ")"
      ],
      "metadata": {
        "id": "3K1WhVWtcTch"
      },
      "id": "3K1WhVWtcTch",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Example 2**"
      ],
      "metadata": {
        "id": "baOdG24TdZk9"
      },
      "id": "baOdG24TdZk9"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
        "response=chain.run(\"Mexican\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dPKcMJglbwyB",
        "outputId": "7374f744-f4aa-4edb-c859-9436ece841da"
      },
      "id": "dPKcMJglbwyB",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-dbadd49ed822>:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  chain = LLMChain(llm=llm, prompt=prompt_template_name)\n",
            "<ipython-input-10-dbadd49ed822>:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response=chain.run(\"Mexican\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mexican restaurant\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LW0hKd11nGfP"
      },
      "id": "LW0hKd11nGfP"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template_advertising = PromptTemplate(\n",
        "    input_variables=['product'],\n",
        "    template=\"I am launching a new {product}. Write a catchy slogan for the advertising campaign.\"\n",
        ")"
      ],
      "metadata": {
        "id": "UYI5KEvznEi6"
      },
      "id": "UYI5KEvznEi6",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=llm, prompt=prompt_template_advertising)\n",
        "response=chain.run(\"AI Course\")\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T6JbpmganEnr",
        "outputId": "923671ca-35ea-4dd7-b241-53272510da47"
      },
      "id": "T6JbpmganEnr",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i am launching a new AI Course\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### When to Use Prompt Templates?\n",
        "Prompt Templates are especially useful in scenarios like:\n",
        "\n",
        "**Translation:** Automatically generate prompts for translating text to various languages.\n",
        "\n",
        "**Question-Answering**: Consistently structure question prompts by inserting dynamic inputs.\n",
        "\n",
        "**Summarization:** Dynamically generate summarization prompts for different texts.\n",
        "\n",
        "**Custom Tasks:** Any task where you need to maintain a consistent structure while inserting variable data, such as in chatbot responses or document generation."
      ],
      "metadata": {
        "id": "8w2h9gYNmLg-"
      },
      "id": "8w2h9gYNmLg-"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FQJaXRsFmcMN"
      },
      "id": "FQJaXRsFmcMN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}