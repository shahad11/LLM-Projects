{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Introduction**\n",
        "\n"
      ],
      "metadata": {
        "id": "T0Vecltn-tEv"
      },
      "id": "T0Vecltn-tEv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **What is Agents in LangChain**\n",
        "An agent in LangChain refers to a component that can **autonomously decide how to handle different tasks by leveraging multiple tools**. It acts like a decision-making entity that can execute sequences of steps (or tasks) based on user inputs and the context. Agents can be either reactive or proactive, depending on how they respond to tasks or anticipate future tasks.\n",
        "\n",
        "Agents in LangChain work by receiving a prompt, deciding which tools (models, APIs, or systems) to use, invoking those tools, and then using the results to continue processing or produce an output.\n",
        "\n",
        "### **Types of Agents in LangChain**\n",
        "\n",
        "**Zero-shot Reactivity Agents:** These agents can decide in real-time how to respond to a new, unseen input. For instance, if you give it a task like summarizing a document or retrieving data, it can automatically choose the right tool.\n",
        "\n",
        "**Conversation Agents:** These agents handle dialogues and choose different models/tools based on the ongoing conversation. They may call APIs, use knowledge bases, or generate text.\n",
        "\n",
        "**Action Agents:** These agents determine when and how to take specific actions by using tools such as search engines, databases, or specialized models to complete tasks.\n",
        "\n",
        "For example, in LangChain:\n",
        "\n",
        "You can build an agent that, given a user question, can search through documents, retrieve relevant information, and summarize it.\n",
        "Agents are often connected to LLMs (like GPT models) that can decide how to process input data and choose the appropriate tools to get tasks done.\n",
        "\n",
        "### **What is  Tools in LangChain **\n",
        "\n",
        "A tool in LangChain refers to any **external resource or service the agent can call to accomplish a specific task**. This could be anything from a language model, database, API, or utility that performs a function (like translation, sentiment analysis, summarization, etc.).\n",
        "\n",
        "### Types of Tools\n",
        "\n",
        "**LLMs (Large Language Models):** These are AI models (like GPT-3, GPT-4) that can generate text, complete tasks, or answer questions based on input prompts.\n",
        "\n",
        "**Search Tools:** Tools like Bing Search, Google Search, or document search engines allow the agent to find relevant information.\n",
        "\n",
        "**APIs:** External services such as weather APIs, financial data APIs, or any other programmatic endpoints agents can interact with.\n",
        "\n",
        "Custom Functions: These could be utilities like Python functions or any custom-built tools that perform a specific task such as mathematical operations, database queries, etc.\n",
        "\n",
        "In Hugging Face, a tool often refers to the pre-trained models or datasets available on their platform. These models could be used for tasks such as text generation, translation, image classification, etc. Hugging Face tools can also be part of LangChain’s toolset when integrated for tasks like NLP, model fine-tuning, or inference.\n",
        "\n",
        "### **How Agents and Tools Work Together in LangChain**\n",
        "**Decision Making with Agents:** The agent first determines what task needs to be done. For example, if a user asks, \"What is the weather in Dubai?\", the agent understands that it should not generate a response directly but should instead call a weather API tool.\n",
        "\n",
        "**Tool Usage:** The agent will then invoke the correct tool (e.g., a weather API or a search engine tool) to fetch the real-time weather data.\n",
        "\n",
        "**Combining Outputs:** If the task is more complex, such as \"Summarize today’s news in Dubai and provide weather information,\"\n",
        "the agent might First, invoke a search tool to get news articles (Tool 1).\n",
        "Second, use a language model to summarize the news (Tool 2).\n",
        "Third, call a weather API to retrieve the forecast (Tool 3).\n"
      ],
      "metadata": {
        "id": "ZwonwDhl-lWB"
      },
      "id": "ZwonwDhl-lWB"
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VqoN0eRC-sGe"
      },
      "id": "VqoN0eRC-sGe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**01: LangChain Installation**"
      ],
      "metadata": {
        "id": "09CgA1RZkiC4"
      },
      "id": "09CgA1RZkiC4"
    },
    {
      "cell_type": "code",
      "source": [
        "#Installing langchain community\n",
        "!pip install langchain langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4tDdLTjkkk_",
        "outputId": "ff405367-e57c-4c00-8e15-fe84a2df7ece"
      },
      "id": "X4tDdLTjkkk_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.8)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.8 (from langchain)\n",
            "  Downloading langchain_core-0.3.8-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.131-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.8->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.8->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.8->langchain) (4.12.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.8->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.3.2-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.3.8-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.131-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, python-dotenv, orjson, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, requests-toolbelt, jsonpatch, httpcore, pydantic-settings, httpx, dataclasses-json, langsmith, langchain-core, langchain-text-splitters, langchain, langchain_community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.2 langchain-core-0.3.8 langchain-text-splitters-0.3.0 langchain_community-0.3.1 langsmith-0.1.131 marshmallow-3.22.0 mypy-extensions-1.0.0 orjson-3.10.7 pydantic-settings-2.5.2 python-dotenv-1.0.1 requests-toolbelt-1.0.0 tenacity-8.5.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#installing HuggingFace\n",
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDMLw7Yr-nQK",
        "outputId": "972e2c3d-7d9b-467e-b6f9-009d334e6ffa"
      },
      "id": "hDMLw7Yr-nQK",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**02: Setup the Environment**"
      ],
      "metadata": {
        "id": "sQHZiF38-Cps"
      },
      "id": "sQHZiF38-Cps"
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "9-mFf0Ql-KX2"
      },
      "id": "9-mFf0Ql-KX2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f31c4cc6",
      "metadata": {
        "id": "f31c4cc6"
      },
      "outputs": [],
      "source": [
        "os.environ['OPENAI_API_KEY'] = \"You OpenAI API key\"\n",
        "\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"Your Hugging face API Key\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4069a75e",
      "metadata": {
        "id": "4069a75e"
      },
      "source": [
        "## **Agents and Tools**\n",
        "\n",
        "Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done.\n",
        "\n",
        "\n",
        "When used correctly agents can be extremely powerful. In order to load agents, you should understand the following concepts:\n",
        "\n",
        "- Tool: A function that performs a specific duty. This can be things like: Google Search, Database lookup, Python REPL, other chains.\n",
        "- LLM: The language model powering the agent.\n",
        "- Agent: The agent to use.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agent is a very powerful concept in LangChain"
      ],
      "metadata": {
        "id": "Z-4QjS31LD_s"
      },
      "id": "Z-4QjS31LD_s"
    },
    {
      "cell_type": "markdown",
      "source": [
        "For example I have to travel from Dubai to Canada, I type this in ChatGPT\n",
        "\n",
        "\n",
        "\n",
        "---> Give me  two flight options from Dubai to Canada on September 1, 2024 | ChatGPT will not be able to answer because has knowledge till\n",
        "September 2021\n",
        "\n",
        "\n",
        "\n",
        "ChatGPT plus has Expedia Plugin, if we enable this plugin it will go to Expedia Plugin and will try to pull information about Flights & it will show the information"
      ],
      "metadata": {
        "id": "GgNLQ6kSL4na"
      },
      "id": "GgNLQ6kSL4na"
    },
    {
      "cell_type": "markdown",
      "source": [
        "SerpApi is a real-time API to access Google search results."
      ],
      "metadata": {
        "id": "tkzApnDnJy8p"
      },
      "id": "tkzApnDnJy8p"
    },
    {
      "cell_type": "markdown",
      "id": "09cd3a12",
      "metadata": {
        "id": "09cd3a12"
      },
      "source": [
        "#### Wikipedia and llm-math tool"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpJ3gA4YZKMx",
        "outputId": "a53dc7a2-730d-4ae4-8b44-d5c4b421e039"
      },
      "id": "TpJ3gA4YZKMx",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading wikipedia-1.4.0.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wikipedia) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2024.8.30)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->wikipedia) (2.6)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-py3-none-any.whl size=11679 sha256=ae7f93e6e0713e9ddd9445e79ec483ce4afabfe8e90616a5589b325e8f605c92\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/b6/c5/93f3dec388ae76edc830cb42901bb0232504dfc0df02fc50de\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import AgentType, initialize_agent, load_tools\n",
        "from langchain.llms import OpenAI"
      ],
      "metadata": {
        "id": "kb7ZpkpcVIYO"
      },
      "id": "kb7ZpkpcVIYO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Choosing an LLM (Language Model) as OpenAI’s Default Model\n",
        "**OpenAI(temperature=0):**\n",
        "This line initializes OpenAI’s language model.\n",
        "\n",
        "**The temperature=0** parameter ensures that the model is deterministic, meaning it will always return the most likely response instead of introducing randomness or creativity. A lower temperature gives more precise and consistent answers, which is important for factual tasks.\n",
        "\n",
        "## 2. Loading the Tools\n",
        "**load_tools():**\n",
        "This function loads specific tools that the agent will have access to for completing tasks.\n",
        "\n",
        "In this case, the agent is given access to two tools:\n",
        "\"**wikipedia**\": A tool that can search Wikipedia to retrieve factual information.\n",
        "\"**llm-math**\": A tool that can solve mathematical problems using a language model (LLM). Since this tool depends on the LLM for processing, the llm is passed as an argument.\n",
        "\n",
        "## 3. Initializing the Agent\n",
        "\n",
        "**agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION:** Specifies the type of agent to use. The Zero-Shot Reactivity agent means that it can decide in real-time how to handle any new input without prior examples. It will react based on the description of the input provided by the user, figuring out whether to use Wikipedia, LLM-math, or both to answer the question.\n",
        "\n",
        "**verbose=True:** If set to True, the agent will print detailed information about each step of its reasoning and tool usage. This is useful for debugging or understanding how the agent makes decisions.\n",
        "\n"
      ],
      "metadata": {
        "id": "AiYJHnkjBhRc"
      },
      "id": "AiYJHnkjBhRc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14d06ce6",
      "metadata": {
        "scrolled": false,
        "id": "14d06ce6",
        "outputId": "eee796fb-5a14-4252-8e8a-7be7207415c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_core.callbacks.manager:Error in StdOutCallbackHandler.on_chain_start callback: AttributeError(\"'NoneType' object has no attribute 'get'\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;1m\u001b[1;3m I should use Wikipedia to find the answer\n",
            "Action: wikipedia\n",
            "Action Input: GDP of US in 2024\u001b[0m\n",
            "Observation: \u001b[36;1m\u001b[1;3mPage: List of U.S. states and territories by GDP\n",
            "Summary: This is a list of U.S. states and territories by gross domestic product (GDP). This article presents the 50 U.S. states and the District of Columbia and their nominal GDP at current prices.\n",
            "The data source for the list is the Bureau of Economic Analysis (BEA) in 2024. The BEA defined GDP by state as \"the sum of value added from all industries in the state.\"\n",
            "Nominal GDP does not take into account differences in the cost of living in different countries, and the results can vary greatly from one year to another based on fluctuations in the exchange rates of the country's currency. Such fluctuations may change a country's ranking from one year to the next, even though they often make little or no difference in the standard of living of its population.\n",
            "Overall, in the calendar year 2024, the United States' Nominal GDP at Current Prices totaled at $28.269 trillion, as compared to $25.744 trillion in 2022.\n",
            "The three U.S. states with the highest GDPs were California ($3.987 trillion), Texas ($2.664 trillion), and New York ($2.226 trillion). The three U.S. states with the lowest GDPs were Vermont ($44.4 billion), Wyoming ($51.4 billion), and Alaska ($69.2 billion).\n",
            "GDP per capita also varied widely throughout the United States in 2024, with New York ($114,380), Massachusetts ($108,185), and North Dakota ($93,983) recording the three highest GDP per capita figures in the U.S., while Mississippi ($50,907), Arkansas ($58,449), and West Virginia ($57,857) recorded the three lowest GDP per capita figures in the U.S. The District of Columbia, though, recorded a GDP per capita figure far higher than any U.S. state in 2024 at $263,220.\n",
            "\n",
            "Page: List of countries by GDP (nominal) per capita\n",
            "Summary: The figures presented here do not take into account differences in the cost of living in different countries, and the results vary greatly from one year to another based on fluctuations in the exchange rates of the country's currency. Such fluctuations change a country's ranking from one year to the next, even though they often make little or no difference to the standard of living of its population.\n",
            "GDP per capita is often considered an indicator of a country's standard of living; however, this is inaccurate because GDP per capita is not a measure of personal income.\n",
            "Comparisons of national income are also frequently made on the basis of purchasing power parity (PPP), to adjust for differences in the cost of living in different countries (see List of countries by GDP (PPP) per capita). PPP largely removes the exchange rate problem but not others; it does not reflect the value of economic output in international trade, and it also requires more estimation than GDP per capita. On the whole, PPP per capita figures are more narrowly spread than nominal GDP per capita figures.\n",
            "Non-sovereign entities (the world, continents, and some dependent territories) and states with limited international recognition are included in the list in cases in which they appear in the sources. These economies are not ranked in the charts here (except Kosovo and Taiwan), but are listed in sequence by GDP for comparison. In addition, non-sovereign entities are marked in italics.\n",
            "Four UN members (Cuba, Liechtenstein, Monaco and North Korea) do not belong to the International Monetary Fund (IMF), hence their economies are not ranked below. Kosovo, despite not being a member of the United Nations, is a member of IMF. Taiwan is not a IMF member but it is still listed in the official IMF indices.\n",
            "Several leading GDP-per-capita (nominal) jurisdictions may be considered tax havens, and their GDP data subject to material distortion by tax-planning activities. Examples include Bermuda, the Cayman Islands, Ireland and Luxembourg.\n",
            "All data are in current United States dollars. Historical data can be found here.\n",
            "\n",
            "Page: List of countries by GDP (nominal)\n",
            "Summary: Gross domestic product (GDP) is the market value of all final goods an\u001b[0m\n",
            "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
            "Final Answer: The GDP of US in 2024 was $28.269 trillion.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The GDP of US in 2024 was $28.269 trillion.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "# Chosing LLm as OpenAI default model\n",
        "llm = OpenAI(temperature=0)\n",
        "# The tools we'll give the Agent access to. Note that the 'llm-math' tool uses an LLM, so we need to pass that in.\n",
        "tools = load_tools([\"wikipedia\", \"llm-math\"], llm=llm)\n",
        "\n",
        "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
        "agent = initialize_agent(\n",
        "    tools,\n",
        "    llm,\n",
        "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Let's test it out!\n",
        "\n",
        "\n",
        "agent.run(\"What was the GDP of US in 2024?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BXtDsAgF8i8S"
      },
      "id": "BXtDsAgF8i8S",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}