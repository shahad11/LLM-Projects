{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **What is Memory In Langchain**\n",
        "\n",
        "In LangChain, memory refers to the **component that enables a language model (LLM) to retain context or information across multiple interactions** or conversations. Unlike traditional LLM interactions, where the model only processes one input at a time and forgets previous context, memory allows the LLM to keep track of what was said earlier in a conversation. This is especially useful for building applications like chatbots, personal assistants, or any system that requires understanding and responding to user inputs based on prior exchanges.\n",
        "\n",
        "Memory in LangChain enhances the capabilities of LLMs by allowing them to remember past interactions, providing context for future responses, and improving the user experience in applications that require ongoing conversations or tasks. Whether you need to store entire conversations, summaries, or specific details, LangChain’s memory types make it easy to implement and tailor memory to your application’s needs.\n",
        "\n",
        "\n",
        "## **Why is Memory Important in LangChain?**\n",
        "Memory in LangChain is critical for maintaining coherence and relevance in multi-step or conversational tasks. It enables the language model to:\n",
        "\n",
        "**Remember past interactions:** So it can provide responses that are contextually aware and consistent across multiple turns in a conversation.\n",
        "\n",
        "**Track user preferences or details:** Such as remembering user names, previous queries, or other specific information during an ongoing session.\n",
        "\n",
        "**Enable multi-step workflows:** By keeping track of intermediate steps or decisions in a process, making it easier to execute complex tasks over time."
      ],
      "metadata": {
        "id": "vMqMDNXMaUmX"
      },
      "id": "vMqMDNXMaUmX"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Types of Memory in LangChain**\n",
        "LangChain supports different types of memory, depending on the use case:\n",
        "\n",
        "**Buffer Memory:**\n",
        "\n",
        "This stores the entire conversation history as a buffer, where each interaction (both user inputs and model responses) is stored in chronological order.\n",
        "Use case: Useful in applications like chatbots where the model needs to remember everything discussed so far.\n",
        "\n",
        "**Summary Memory:**\n",
        "\n",
        "Instead of storing the entire conversation, this memory periodically summarizes the conversation and stores the summary. This is efficient when you need context but don't want to overload the model with long conversations.\n",
        "Use case: Useful when the conversation gets long and it's impractical to store everything.\n",
        "\n",
        "**Vector-based Memory:**\n",
        "\n",
        "This type of memory stores chunks of information as vectors using embeddings, which allows the model to retrieve relevant parts of the conversation without keeping the entire history in plain text.\n",
        "Use case: Useful in knowledge retrieval tasks where specific details from past interactions need to be recalled with semantic understanding.\n",
        "\n",
        "**Key-Value Memory:**\n",
        "\n",
        "This stores specific pieces of information as key-value pairs. It’s useful for remembering specific facts or details like a user’s preferences, names, or past inputs.\n",
        "Use case: This is perfect for structured interactions where only certain details need to be retained, such as a personal assistant remembering a user’s preferences.\n",
        "\n",
        "## **How Memory Works in LangChain**\n",
        "\n",
        "Memory in LangChain works by storing relevant information from past interactions and injecting that stored information into the prompts that are sent to the language model. Depending on the memory type, LangChain may store the entire conversation, summarized context, or specific key-value pairs, ensuring that the model maintains awareness of previous exchanges.\n",
        "\n",
        "### **Use Cases of Memory in LangChain**\n",
        "\n",
        "**Chatbots:** Memory allows chatbots to hold long conversations while retaining important information.\n",
        "\n",
        "**Personal Assistants:** For remembering user preferences, scheduling details, or previous interactions.\n",
        "\n",
        "**Customer Support:** Retain customer history to provide better, more informed responses based on past issues or queries.\n",
        "\n",
        "**Interactive Storytelling:** In AI-driven storytelling, memory allows the model to recall previous events in the story and keep the narrative coherent."
      ],
      "metadata": {
        "id": "0rmv2Ppka5lk"
      },
      "id": "0rmv2Ppka5lk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**01: Installation**"
      ],
      "metadata": {
        "id": "09CgA1RZkiC4"
      },
      "id": "09CgA1RZkiC4"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain_community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4tDdLTjkkk_",
        "outputId": "c6c0fb4a-b34e-439a-82e6-0786342db532"
      },
      "id": "X4tDdLTjkkk_",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.8)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.8 (from langchain)\n",
            "  Downloading langchain_core-0.3.9-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.131-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.8->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.8->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.8->langchain) (4.12.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.8->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.3.2-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m37.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading langchain_core-0.3.9-py3-none-any.whl (401 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m401.8/401.8 kB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.131-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, python-dotenv, orjson, mypy-extensions, marshmallow, jsonpointer, h11, typing-inspect, requests-toolbelt, jsonpatch, httpcore, pydantic-settings, httpx, dataclasses-json, langsmith, langchain-core, langchain-text-splitters, langchain, langchain_community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.2 langchain-core-0.3.9 langchain-text-splitters-0.3.0 langchain_community-0.3.1 langsmith-0.1.131 marshmallow-3.22.0 mypy-extensions-1.0.0 orjson-3.10.7 pydantic-settings-2.5.2 python-dotenv-1.0.1 requests-toolbelt-1.0.0 tenacity-8.5.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**02: Setup the Environment**"
      ],
      "metadata": {
        "id": "sQHZiF38-Cps"
      },
      "id": "sQHZiF38-Cps"
    },
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "9-mFf0Ql-KX2"
      },
      "id": "9-mFf0Ql-KX2",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f31c4cc6",
      "metadata": {
        "id": "f31c4cc6"
      },
      "outputs": [],
      "source": [
        "# Create your API key from https://huggingface.co/settings/tokens\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"Your API KEY\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Hugging Face**"
      ],
      "metadata": {
        "id": "bj6wjnKZ-bgU"
      },
      "id": "bj6wjnKZ-bgU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Example 1**"
      ],
      "metadata": {
        "id": "iTsUW116-th1"
      },
      "id": "iTsUW116-th1"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDMLw7Yr-nQK",
        "outputId": "f4918e55-d85a-4b9d-e4fb-0b1d90a9206f"
      },
      "id": "hDMLw7Yr-nQK",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import HuggingFaceHub"
      ],
      "metadata": {
        "id": "B4w0ultA-icd"
      },
      "id": "B4w0ultA-icd",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b6be7ee7",
      "metadata": {
        "id": "b6be7ee7"
      },
      "source": [
        "##**3: Memory in LangChain**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chatbot application like ChatGPT, you will notice that it remember past information"
      ],
      "metadata": {
        "id": "-WkJqQzRZaXL"
      },
      "id": "-WkJqQzRZaXL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HuggingFaceHub:**\n",
        "\n",
        "HuggingFaceHub is a utility in LangChain that **allows you to load pre-trained language models** from Hugging Face's model repository.\n",
        "Here, you're using the HuggingFaceHub to initialize the **google/flan-t5-large** model from Hugging Face's collection.\n",
        "repo_id=\"google/flan-t5-large\":\n",
        "\n",
        "**repo_id specifies** which model to use from Hugging Face's model hub. In this case, it's google/flan-t5-large, a large T5 model fine-tuned by Google for a variety of natural language processing (NLP) tasks, such as text generation, translation, and summarization.\n",
        "\n",
        "**model_kwargs={\"temperature\": 0.8, \"max_length\": 64}:**\n",
        "\n",
        "This parameter contains additional settings for configuring the behavior of the language model during generation.\n",
        "\n",
        "**temperature=0.8:**\n",
        "\n",
        "The temperature parameter controls the creativity or randomness of the model's output. It affects how the model decides between possible next words in the generated text.\n",
        "A temperature of 1 means the model will choose words more randomly, while a lower temperature (like 0.2) makes it more deterministic. A value of 0.8 strikes a balance between randomness and control, encouraging some creative variety in responses while still maintaining coherence.\n",
        "\n",
        "**max_length=64:**\n",
        "\n",
        "max_length determines the maximum number of tokens (words or word fragments) the model is allowed to generate in its response. Here, it’s set to 64 tokens. This ensures that the model doesn't generate excessively long responses."
      ],
      "metadata": {
        "id": "1sLNwzJscuUb"
      },
      "id": "1sLNwzJscuUb"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "llm = HuggingFaceHub(repo_id=\"google/flan-t5-large\", model_kwargs={\"temperature\":0.8, \"max_length\":64})"
      ],
      "metadata": {
        "id": "Iqunzha0Ztuz"
      },
      "id": "Iqunzha0Ztuz",
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2kJHMxwAYyZ",
        "outputId": "846ab4ca-b54e-4037-8e4e-5796ae3f6f45"
      },
      "id": "H2kJHMxwAYyZ",
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HuggingFaceHub(client=<InferenceClient(model='google/flan-t5-base', timeout=None)>, repo_id='google/flan-t5-base', task='text2text-generation', model_kwargs={'temperature': 0.8, 'max_length': 64})"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PromptTemplate:**\n",
        "This is a class from the LangChain library that allows you to create customizable prompt templates. These templates can include placeholders (variables) that will be filled dynamically based on user input or specific data.\n",
        "\n",
        "**input_variables=['cuisine']:**\n",
        "\n",
        "input_variables specifies the dynamic part(s) of the template. Here, it indicates that the template contains a variable called cuisine.\n",
        "When using this template, you’ll need to provide a value for the cuisine variable (e.g., Italian, Chinese, Mexican, etc.).\n",
        "\n",
        "**template=**\"I want to open a restaurant for {cuisine} food..:\n",
        "\n",
        "This is the actual prompt template text. The {cuisine} is a placeholder that will be replaced with the specific cuisine provided at runtime."
      ],
      "metadata": {
        "id": "gWs4q0khdMAk"
      },
      "id": "gWs4q0khdMAk"
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "prompt_template_name = PromptTemplate(\n",
        "    input_variables =['cuisine'],\n",
        "    template = \"I want to open a restaurant for {cuisine} food. Suggest a fency name for this.\"\n",
        ")"
      ],
      "metadata": {
        "id": "NE-poGM1Zxss"
      },
      "id": "NE-poGM1Zxss",
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLMChain:**\n",
        "This is a class from the LangChain library that represents a chain involving a language model (LLM) and a prompt. The chain processes inputs through the language model to generate a response.\n",
        "\n",
        "**chain.run(\"Mexican\"):**\n",
        "\n",
        "The **.run()** method runs the chain by taking the input (\"Mexican\") and passing it through the prompt template and language model."
      ],
      "metadata": {
        "id": "SWnPQ7qfdxNo"
      },
      "id": "SWnPQ7qfdxNo"
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "id": "2acab5d0",
      "metadata": {
        "id": "2acab5d0",
        "outputId": "c6cd5f44-3cda-474c-b84a-c3e5eb6db5fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "el teo\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import LLMChain\n",
        "\n",
        "chain = LLMChain(llm=llm,prompt=prompt_template_name)\n",
        "name = chain.run(\"Mexican\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "5bc200f9",
      "metadata": {
        "id": "5bc200f9",
        "outputId": "b04178c8-5bf8-4739-9875-968161579084",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "saada\n"
          ]
        }
      ],
      "source": [
        "name = chain.run(\"Indian\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "229a6888",
      "metadata": {
        "id": "229a6888"
      },
      "outputs": [],
      "source": [
        "chain.memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "f492fb5a",
      "metadata": {
        "scrolled": true,
        "id": "f492fb5a",
        "outputId": "fbb7d8a7-4447-4b40-b948-ab60d343f886",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NoneType"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ],
      "source": [
        "# Here we can see there is no memory of previous inputs\n",
        "type(chain.memory)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "871492be",
      "metadata": {
        "id": "871492be"
      },
      "source": [
        "##**ConversationBufferMemory**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can attach memory to remember all previous conversation"
      ],
      "metadata": {
        "id": "coQKpk8jZ8zz"
      },
      "id": "coQKpk8jZ8zz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ConversationBufferMemory:**\n",
        "This is a type of memory in LangChain that stores the entire conversation history (both user inputs and model responses) in a buffer. The memory keeps track of all previous interactions, allowing the language model to maintain context across multiple turns in the conversation."
      ],
      "metadata": {
        "id": "MIJcaaSaeR0N"
      },
      "id": "MIJcaaSaeR0N"
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "53eea298",
      "metadata": {
        "id": "53eea298",
        "outputId": "f6a68518-319e-4e57-9077-3cc357975029",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "el teo\n"
          ]
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "\n",
        "memory = ConversationBufferMemory()\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template_name, memory=memory)\n",
        "name = chain.run(\"Mexican\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "0de5d50b",
      "metadata": {
        "id": "0de5d50b",
        "outputId": "34e18ede-8b5f-4371-efe9-038741921a15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ahmed ahmed\n"
          ]
        }
      ],
      "source": [
        "name = chain.run(\"Arabic\")\n",
        "print(name)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**chain.memory.buffer:**\n",
        "This is accessing the conversation buffer stored in the ConversationBufferMemory object.\n",
        "The buffer contains the entire history of the conversation, including all user inputs and the corresponding responses from the language model."
      ],
      "metadata": {
        "id": "w3nVKADyeh7F"
      },
      "id": "w3nVKADyeh7F"
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "id": "5cc88888",
      "metadata": {
        "scrolled": true,
        "id": "5cc88888",
        "outputId": "ce92a727-747e-40e0-a46f-c492aff1ec8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Mexican\n",
            "AI: el teo\n",
            "Human: Arabic\n",
            "AI: ahmed ahmed\n"
          ]
        }
      ],
      "source": [
        "print(chain.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0a88b5b",
      "metadata": {
        "id": "a0a88b5b"
      },
      "source": [
        "##**ConversationChain**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conversation buffer memory goes growing endlessly\n",
        "\n",
        "Just remember last 5 Conversation Chain\n",
        "\n",
        "Just remember last 10-20 Conversation Chain"
      ],
      "metadata": {
        "id": "FyFmOOemaVxb"
      },
      "id": "FyFmOOemaVxb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ConversationChain:**\n",
        "\n",
        "\n",
        "ConversationChain is designed to facilitate multi-turn conversations with a language model, allowing the model to maintain context across multiple interactions."
      ],
      "metadata": {
        "id": "4JuQwSPRevzm"
      },
      "id": "4JuQwSPRevzm"
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "id": "687ddd2f",
      "metadata": {
        "id": "687ddd2f",
        "outputId": "75ba22a1-4473-4c25-d16f-5def2137c67a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
            "\n",
            "Current conversation:\n",
            "{history}\n",
            "Human: {input}\n",
            "AI:\n"
          ]
        }
      ],
      "source": [
        "from langchain.chains import ConversationChain\n",
        "\n",
        "convo = ConversationChain(llm=llm)\n",
        "print(convo.prompt.template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "id": "47ad5062",
      "metadata": {
        "id": "47ad5062",
        "outputId": "7c5d0a3d-71b0-4b44-8beb-d5dc8bfaade0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The first T20 cricket world cup was won by India in 2003.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 169
        }
      ],
      "source": [
        "convo.run(\"Who won the first T20 cricket world cup?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "id": "03c80b54",
      "metadata": {
        "id": "03c80b54",
        "outputId": "70942dfe-cb75-46e4-af1a-5f667f00353b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'5 is the number of 5 added with 5.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 170
        }
      ],
      "source": [
        "convo.run(\"what is 5 added with 5?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "id": "07342f88",
      "metadata": {
        "id": "07342f88",
        "outputId": "ffe0b47c-aa8e-4551-fddf-5dcd80eb6608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The captain of the winning team was Virat Kohli.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 171
        }
      ],
      "source": [
        "convo.run(\"Who was the captain of the winning team?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 172,
      "id": "4e459d07",
      "metadata": {
        "id": "4e459d07",
        "outputId": "6f07f9c0-ad94-46d7-f235-757047d24b98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Who won the first T20 cricket world cup?\n",
            "AI: The first T20 cricket world cup was won by India in 2003.\n",
            "Human: what is 5 added with 5?\n",
            "AI: 5 is the number of 5 added with 5.\n",
            "Human: Who was the captain of the winning team?\n",
            "AI: The captain of the winning team was Virat Kohli.\n"
          ]
        }
      ],
      "source": [
        "print(convo.memory.buffer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "feaa3abd",
      "metadata": {
        "id": "feaa3abd"
      },
      "source": [
        "##**ConversationBufferWindowMemory**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ConversationBufferWindowMemory(k=4):**\n",
        "This initializes a memory object that retains the last 4 interactions (both user inputs and model responses) in the conversation.\n",
        "The parameter k=4 specifies the size of the window, meaning that at any given point, only the last 4 conversational exchanges will be remembered. This is useful when you only need short-term context instead of remembering the entire conversation."
      ],
      "metadata": {
        "id": "nNGK8gD1fXdP"
      },
      "id": "nNGK8gD1fXdP"
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "id": "460eb33c",
      "metadata": {
        "id": "460eb33c",
        "outputId": "b018cbe3-6aab-41a1-d8fa-3ed313267bd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The last soccer world cup was in Brazil in 2018'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 164
        }
      ],
      "source": [
        "from langchain.memory import ConversationBufferWindowMemory\n",
        "\n",
        "memory = ConversationBufferWindowMemory(k=4)\n",
        "\n",
        "convo = ConversationChain(\n",
        "    llm=llm,\n",
        "    memory=memory\n",
        ")\n",
        "convo.run(\"Who won the last soccer world cup?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "id": "d395beaf",
      "metadata": {
        "id": "d395beaf",
        "outputId": "711fd395-120d-427b-d813-32a01ca1fa80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Messi is a footballer.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 162
        }
      ],
      "source": [
        "convo.run(\"who is Messi?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "id": "93b24745",
      "metadata": {
        "id": "93b24745",
        "outputId": "2dd633c5-39c7-4343-b39d-59aa2629b4e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The captain of the winning team was Lionel Messi'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 165
        }
      ],
      "source": [
        "convo.run(\"Who was the captain of the winning team?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "convo.run(\"Which was the other team?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WZxtrkv-EBZz",
        "outputId": "338eaa9f-4d7e-4e67-fc64-aaae59ee66af"
      },
      "id": "WZxtrkv-EBZz",
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The other team was Germany'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 166
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(convo.memory.buffer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K63Ie5FTvjzo",
        "outputId": "67a7a681-9018-4997-e04d-47cad8109fe2"
      },
      "id": "K63Ie5FTvjzo",
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Who won the last soccer world cup?\n",
            "AI: The last soccer world cup was in Brazil in 2018\n",
            "Human: Who was the captain of the winning team?\n",
            "AI: The captain of the winning team was Lionel Messi\n",
            "Human: Which was the other team?\n",
            "AI: The other team was Germany\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Note:- This is a notebook to practice memory in langchain. There may have some factually incorrect output from model, I urge you to try out other models like openAI etc.."
      ],
      "metadata": {
        "id": "PxxyMmSfgCo-"
      },
      "id": "PxxyMmSfgCo-"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "77KUe0qSmt5h"
      },
      "id": "77KUe0qSmt5h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bduwbig1mt9c"
      },
      "id": "bduwbig1mt9c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FSnv3gV3muBm"
      },
      "id": "FSnv3gV3muBm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uO4dx_pNmuFt"
      },
      "id": "uO4dx_pNmuFt",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}